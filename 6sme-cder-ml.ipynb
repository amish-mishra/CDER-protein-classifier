{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SME+CDER models and ML classifier  \n",
    "Author: Amish Mishra  \n",
    "Date: April 8, 2023  \n",
    "Use `cder2` kernel  \n",
    "This notebook loads in the PDs for the proteins, trains CDER models for each topology to distinguish stable from unstable proteins, and then trains an ML classifier on the CDER coordinate evaluations on a test set and the SME features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas\n",
    "import multidim\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from multidim.models import CDER\n",
    "from multidim.covertree import CoverTree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.stats as sps\n",
    "from sklearn import model_selection\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_threshold_labeller(label_col_value, thresh=1.0):\n",
    "    '''\n",
    "    Takes a stability score and returns a lable for it depending on a threshold\n",
    "    '''\n",
    "    if label_col_value > thresh:\n",
    "        return \"green\"\n",
    "    else:\n",
    "        return \"red\"\n",
    "    \n",
    "\n",
    "def preprocess_pd(pd, skew=False, filter='top', t=10):\n",
    "    \"\"\"\n",
    "    Transform a diagram into birth-death coordinates and/or filter out low persistence pairs\n",
    "    :param pd: Nx2 numpy array of persistence pairs\n",
    "    :param skew: boolean flag to return point clouds in birth-death coordinates or birth-persistence coordinates\n",
    "    :param filter: string specifying the type of filtering to do\n",
    "                   'top': retain at most the t most persistent pairs,\n",
    "                   'threshold': retain pairs with persistence above t\n",
    "    :param t: value of filtering parameter\n",
    "    :return: the processed pd, the number of pairs retained\n",
    "    \"\"\"\n",
    "    num_pairs = None\n",
    "    if pd.shape[1] == 1:  # This is the case for the H_0 persistence pairs which are 1-dim arrays\n",
    "        pers = pd[:, 0]\n",
    "    else:\n",
    "        pers = pd[:, 1] - pd[:, 0]\n",
    "        \n",
    "    if filter == 'threshold':\n",
    "        pd_new = pd[pers > t, :]\n",
    "    elif filter == 'all':\n",
    "        pd_new = pd\n",
    "        num_pairs = len(pd)\n",
    "    elif filter == 'top':\n",
    "        pd_new = pd[np.argsort(pers), :]\n",
    "        if len(pers) > t:\n",
    "            pd_new = pd_new[len(pers)-t:, :]\n",
    "        num_pairs = t\n",
    "    elif filter == 'random_pairs':\n",
    "        number_of_rows = pd.shape[0]\n",
    "        random_indices = np.random.choice(number_of_rows, size=t, replace=False)\n",
    "        pd_new = pd[random_indices, :]\n",
    "    elif filter == 'random_percent':\n",
    "        number_of_rows = pd.shape[0]\n",
    "        num_pairs = int(np.ceil(number_of_rows*t))\n",
    "        random_indices = np.random.choice(number_of_rows, size=num_pairs, replace=False)\n",
    "        pd_new = pd[random_indices, :]\n",
    "\n",
    "    # convert pd to (birth, persistence) pairs\n",
    "    if skew and pd.shape[1] != 1:\n",
    "        pd_new[:, 1] = pd_new[:, 1] - pd_new[:, 0]\n",
    "    return pd_new, num_pairs\n",
    "\n",
    "\n",
    "def preprocess_pd_array(pd_array, skew=False, filter='top', t=10):\n",
    "    \"\"\"\n",
    "    Transform the diagrams contained in a diagram array whose keys are equal to homological dimension\n",
    "    into birth-death coordinates and/or filter out low persistence pairs\n",
    "    :param pd_array: array of persistence diagrams in a fixed dimention, i.e. value in one entry in the \n",
    "                     the dictionary output of load_pd_pcs()\n",
    "    :param skew: boolean flag to return point clouds in birth-death coordinates or birth-persistence coordinates\n",
    "    :param filter: string specifying the type of filtering to do \n",
    "                   'top': retain at most the t most persistent pairs,\n",
    "                   'threshold': retain pairs with persistence above t\n",
    "    :param t: value of filtering parameter\n",
    "    :return: the processed pd array, the number of pairs retained in each pd\n",
    "    \"\"\"\n",
    "    pd_array_new = []\n",
    "    num_pairs_arr = []\n",
    "    # loop over each diagram in the list of diagrams in this dimension and process\n",
    "    for pd in pd_array:\n",
    "        filtered_pd, num_pairs_retained = preprocess_pd(pd, skew=skew, filter=filter, t=t)\n",
    "        pd_array_new.append(filtered_pd)\n",
    "        num_pairs_arr.append(num_pairs_retained)\n",
    "    \n",
    "    return pd_array_new, num_pairs_arr\n",
    "\n",
    "\n",
    "def gen_cder_model(clouds, labels, stop_level=None, parsimonious=True, full_ct=True, out_file=None):\n",
    "    \"\"\"\n",
    "    Generates and optionally saves a cder model\n",
    "    :param clouds: Nx1 list of numpy arrays of point clouds\n",
    "    :param labels: Nx1 list of labels of the point clouds\n",
    "    :param stop_level: integer specifying the cover tree level at which to stop (only applies if full_ct=False)\n",
    "    :param parsimonious: boolean flag specifying parsimonious and full cder model\n",
    "    :param full_ct: boolean flag specifying if full cover tree should be built\n",
    "    :param out_file: path to save the cder model and point cloud data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if full_ct:\n",
    "        cder_model = CDER(parsimonious=parsimonious)\n",
    "        pc_train = multidim.PointCloud.from_multisample_multilabel(clouds, labels)\n",
    "        ct_train = CoverTree(pc_train)\n",
    "        cder_model.fit(ct_train)\n",
    "    else:\n",
    "        cder_model = CDER(stop_level=stop_level, parsimonious=parsimonious)\n",
    "        ct_train = None\n",
    "        cder_model.fit(clouds, labels)\n",
    "\n",
    "    if out_file is not None:\n",
    "        with open(out_file, 'wb') as f:\n",
    "            pickle.dump((clouds, labels, ct_train, cder_model), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return ct_train, cder_model\n",
    "\n",
    "\n",
    "def round_array(arr):\n",
    "    '''define a function to round each element of an array to two decimal places'''\n",
    "    return np.round(arr, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the dataframe with protein information  \n",
    "red: unstable  \n",
    "green: stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topology</th>\n",
       "      <th>stabilityscore_cnn_calibrated</th>\n",
       "      <th>label</th>\n",
       "      <th>pd_path</th>\n",
       "      <th>name</th>\n",
       "      <th>AlaCount</th>\n",
       "      <th>T1_absq</th>\n",
       "      <th>T1_netq</th>\n",
       "      <th>Tend_absq</th>\n",
       "      <th>Tend_netq</th>\n",
       "      <th>...</th>\n",
       "      <th>res_count_core_SCN</th>\n",
       "      <th>score_per_res</th>\n",
       "      <th>ss_contributes_core</th>\n",
       "      <th>ss_sc</th>\n",
       "      <th>sum_best_frags</th>\n",
       "      <th>total_score</th>\n",
       "      <th>tryp_cut_sites</th>\n",
       "      <th>two_core_each</th>\n",
       "      <th>worst6frags</th>\n",
       "      <th>worstfrag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HHH</td>\n",
       "      <td>0.542808</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/HHH_rd1_0825.pkl</td>\n",
       "      <td>HHH_rd1_0825</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.635028</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.764841</td>\n",
       "      <td>5.3591</td>\n",
       "      <td>-113.306191</td>\n",
       "      <td>15</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.5963</td>\n",
       "      <td>0.3444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EEHEE</td>\n",
       "      <td>1.687863</td>\n",
       "      <td>green</td>\n",
       "      <td>./protein_pds/EEHEE_rd4_0226.pkl</td>\n",
       "      <td>EEHEE_rd4_0226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-3.385700</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.786320</td>\n",
       "      <td>9.6969</td>\n",
       "      <td>-145.585091</td>\n",
       "      <td>6</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3.0023</td>\n",
       "      <td>0.5696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HEEH</td>\n",
       "      <td>-0.325246</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/HEEH_rd2_0035.pkl</td>\n",
       "      <td>HEEH_rd2_0035</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-2.240384</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.764385</td>\n",
       "      <td>12.5276</td>\n",
       "      <td>-96.336497</td>\n",
       "      <td>11</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4.6847</td>\n",
       "      <td>1.1061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EHEE</td>\n",
       "      <td>0.244920</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/EHEE_rd3_0179.pkl</td>\n",
       "      <td>EHEE_rd3_0179</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.048340</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.781405</td>\n",
       "      <td>8.5412</td>\n",
       "      <td>-81.933599</td>\n",
       "      <td>8</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.8469</td>\n",
       "      <td>0.5045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EEHEE</td>\n",
       "      <td>0.985595</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/EEHEE_rd3_1627.pkl</td>\n",
       "      <td>EEHEE_rd3_1627</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.640597</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.764510</td>\n",
       "      <td>9.7160</td>\n",
       "      <td>-113.545656</td>\n",
       "      <td>6</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3.0599</td>\n",
       "      <td>0.6737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16169</th>\n",
       "      <td>EEHEE</td>\n",
       "      <td>-0.042749</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/EEHEE_rd3_0672.pkl</td>\n",
       "      <td>EEHEE_rd3_0672</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.339757</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.766706</td>\n",
       "      <td>13.7124</td>\n",
       "      <td>-100.609572</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.4130</td>\n",
       "      <td>0.8748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16170</th>\n",
       "      <td>HEEH</td>\n",
       "      <td>-0.037665</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/HEEH_rd1_0808.pkl</td>\n",
       "      <td>HEEH_rd1_0808</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-2.275365</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.680462</td>\n",
       "      <td>22.1786</td>\n",
       "      <td>-97.840676</td>\n",
       "      <td>12</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>9.0900</td>\n",
       "      <td>2.0749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16171</th>\n",
       "      <td>HHH</td>\n",
       "      <td>1.815568</td>\n",
       "      <td>green</td>\n",
       "      <td>./protein_pds/HHH_rd3_0061.pkl</td>\n",
       "      <td>HHH_rd3_0061</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.766525</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.811374</td>\n",
       "      <td>6.0599</td>\n",
       "      <td>-118.960588</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.5719</td>\n",
       "      <td>0.2963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16172</th>\n",
       "      <td>EEHEE</td>\n",
       "      <td>0.681623</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/EEHEE_rd4_0744.pkl</td>\n",
       "      <td>EEHEE_rd4_0744</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.088340</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.762311</td>\n",
       "      <td>10.8362</td>\n",
       "      <td>-132.798628</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>3.3353</td>\n",
       "      <td>0.6535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16173</th>\n",
       "      <td>HEEH</td>\n",
       "      <td>-0.203491</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/HEEH_rd1_0631.pkl</td>\n",
       "      <td>HEEH_rd1_0631</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.381957</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.773326</td>\n",
       "      <td>16.0268</td>\n",
       "      <td>-102.424158</td>\n",
       "      <td>12</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>6.7084</td>\n",
       "      <td>1.5133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16174 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      topology  stabilityscore_cnn_calibrated  label  \\\n",
       "0          HHH                       0.542808    red   \n",
       "1        EEHEE                       1.687863  green   \n",
       "2         HEEH                      -0.325246    red   \n",
       "3         EHEE                       0.244920    red   \n",
       "4        EEHEE                       0.985595    red   \n",
       "...        ...                            ...    ...   \n",
       "16169    EEHEE                      -0.042749    red   \n",
       "16170     HEEH                      -0.037665    red   \n",
       "16171      HHH                       1.815568  green   \n",
       "16172    EEHEE                       0.681623    red   \n",
       "16173     HEEH                      -0.203491    red   \n",
       "\n",
       "                                pd_path            name  AlaCount  T1_absq  \\\n",
       "0        ./protein_pds/HHH_rd1_0825.pkl    HHH_rd1_0825       6.0      9.0   \n",
       "1      ./protein_pds/EEHEE_rd4_0226.pkl  EEHEE_rd4_0226       4.0      2.0   \n",
       "2       ./protein_pds/HEEH_rd2_0035.pkl   HEEH_rd2_0035       3.0      5.0   \n",
       "3       ./protein_pds/EHEE_rd3_0179.pkl   EHEE_rd3_0179       4.0      1.0   \n",
       "4      ./protein_pds/EEHEE_rd3_1627.pkl  EEHEE_rd3_1627       4.0      1.0   \n",
       "...                                 ...             ...       ...      ...   \n",
       "16169  ./protein_pds/EEHEE_rd3_0672.pkl  EEHEE_rd3_0672       5.0      1.0   \n",
       "16170   ./protein_pds/HEEH_rd1_0808.pkl   HEEH_rd1_0808       6.0      3.0   \n",
       "16171    ./protein_pds/HHH_rd3_0061.pkl    HHH_rd3_0061       6.0      5.0   \n",
       "16172  ./protein_pds/EEHEE_rd4_0744.pkl  EEHEE_rd4_0744       1.0      0.0   \n",
       "16173   ./protein_pds/HEEH_rd1_0631.pkl   HEEH_rd1_0631       4.0      5.0   \n",
       "\n",
       "       T1_netq  Tend_absq  Tend_netq  ...  res_count_core_SCN  score_per_res  \\\n",
       "0         -5.0       13.0        9.0  ...                 5.0      -2.635028   \n",
       "1         -2.0        4.0        2.0  ...                 7.0      -3.385700   \n",
       "2         -5.0        9.0        7.0  ...                 8.0      -2.240384   \n",
       "3          1.0        3.0       -1.0  ...                 5.0      -2.048340   \n",
       "4         -1.0        3.0        3.0  ...                 5.0      -2.640597   \n",
       "...        ...        ...        ...  ...                 ...            ...   \n",
       "16169     -1.0        3.0        1.0  ...                 7.0      -2.339757   \n",
       "16170     -3.0        7.0        3.0  ...                 6.0      -2.275365   \n",
       "16171     -1.0        8.0        2.0  ...                 3.0      -2.766525   \n",
       "16172      0.0        2.0        2.0  ...                 8.0      -3.088340   \n",
       "16173     -5.0       11.0        5.0  ...                 5.0      -2.381957   \n",
       "\n",
       "       ss_contributes_core     ss_sc  sum_best_frags  total_score  \\\n",
       "0                     1.00  0.764841          5.3591  -113.306191   \n",
       "1                     1.00  0.786320          9.6969  -145.585091   \n",
       "2                     1.00  0.764385         12.5276   -96.336497   \n",
       "3                     1.00  0.781405          8.5412   -81.933599   \n",
       "4                     1.00  0.764510          9.7160  -113.545656   \n",
       "...                    ...       ...             ...          ...   \n",
       "16169                 1.00  0.766706         13.7124  -100.609572   \n",
       "16170                 0.75  0.680462         22.1786   -97.840676   \n",
       "16171                 1.00  0.811374          6.0599  -118.960588   \n",
       "16172                 1.00  0.762311         10.8362  -132.798628   \n",
       "16173                 1.00  0.773326         16.0268  -102.424158   \n",
       "\n",
       "       tryp_cut_sites  two_core_each  worst6frags  worstfrag  \n",
       "0                  15       0.333333       1.5963     0.3444  \n",
       "1                   6       0.200000       3.0023     0.5696  \n",
       "2                  11       0.750000       4.6847     1.1061  \n",
       "3                   8       0.250000       2.8469     0.5045  \n",
       "4                   6       0.200000       3.0599     0.6737  \n",
       "...               ...            ...          ...        ...  \n",
       "16169               6       0.600000       4.4130     0.8748  \n",
       "16170              12       0.500000       9.0900     2.0749  \n",
       "16171               9       0.000000       1.5719     0.2963  \n",
       "16172               6       0.600000       3.3353     0.6535  \n",
       "16173              12       0.250000       6.7084     1.5133  \n",
       "\n",
       "[16174 rows x 114 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pandas.read_csv('main_df.csv')\n",
    "topologies_arr = raw_df['topology'].unique()  # store the different topology types\n",
    "\n",
    "# Label stable: green and unstable: red\n",
    "stability_threshold = 1.0  # set the stability score at which to separate the stable label from the unstable label\n",
    "label_col = raw_df['stabilityscore_cnn_calibrated'].apply(stability_threshold_labeller, thresh = stability_threshold)\n",
    "raw_df.insert(loc=2, column='label', value=label_col)  # insert new label column in the 2nd index position\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample the unstable and stable proteins \n",
    "Select the same number of unstable and stable proteins based on a threshold for the stable proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing 1346 stable designs out of 1346 for HHH\n",
      "Choosing 1123 unstable designs out of 1123 for HHH\n",
      "Choosing 579 stable designs out of 579 for EEHEE\n",
      "Choosing 579 unstable designs out of 4669 for EEHEE\n",
      "Choosing 118 stable designs out of 118 for HEEH\n",
      "Choosing 118 unstable designs out of 4872 for HEEH\n",
      "Choosing 679 stable designs out of 679 for EHEE\n",
      "Choosing 679 unstable designs out of 2788 for EHEE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topology</th>\n",
       "      <th>stabilityscore_cnn_calibrated</th>\n",
       "      <th>label</th>\n",
       "      <th>pd_path</th>\n",
       "      <th>name</th>\n",
       "      <th>AlaCount</th>\n",
       "      <th>T1_absq</th>\n",
       "      <th>T1_netq</th>\n",
       "      <th>Tend_absq</th>\n",
       "      <th>Tend_netq</th>\n",
       "      <th>...</th>\n",
       "      <th>res_count_core_SCN</th>\n",
       "      <th>score_per_res</th>\n",
       "      <th>ss_contributes_core</th>\n",
       "      <th>ss_sc</th>\n",
       "      <th>sum_best_frags</th>\n",
       "      <th>total_score</th>\n",
       "      <th>tryp_cut_sites</th>\n",
       "      <th>two_core_each</th>\n",
       "      <th>worst6frags</th>\n",
       "      <th>worstfrag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HHH</td>\n",
       "      <td>2.675084</td>\n",
       "      <td>green</td>\n",
       "      <td>./protein_pds/HHH_rd4_0122.pkl</td>\n",
       "      <td>HHH_rd4_0122</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.233831</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.778585</td>\n",
       "      <td>5.9791</td>\n",
       "      <td>-139.054743</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.6708</td>\n",
       "      <td>0.3360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HHH</td>\n",
       "      <td>2.663947</td>\n",
       "      <td>green</td>\n",
       "      <td>./protein_pds/HHH_rd4_0395.pkl</td>\n",
       "      <td>HHH_rd4_0395</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-3.185288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.763438</td>\n",
       "      <td>5.4102</td>\n",
       "      <td>-136.967366</td>\n",
       "      <td>13</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.4301</td>\n",
       "      <td>0.2915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HHH</td>\n",
       "      <td>2.614201</td>\n",
       "      <td>green</td>\n",
       "      <td>./protein_pds/HHH_rd4_0616.pkl</td>\n",
       "      <td>HHH_rd4_0616</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.044371</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810521</td>\n",
       "      <td>4.7901</td>\n",
       "      <td>-130.907940</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.3618</td>\n",
       "      <td>0.2776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HHH</td>\n",
       "      <td>2.613560</td>\n",
       "      <td>green</td>\n",
       "      <td>./protein_pds/HHH_rd4_0228.pkl</td>\n",
       "      <td>HHH_rd4_0228</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-3.183216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800401</td>\n",
       "      <td>5.7477</td>\n",
       "      <td>-136.878274</td>\n",
       "      <td>11</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.7759</td>\n",
       "      <td>0.3556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HHH</td>\n",
       "      <td>2.610852</td>\n",
       "      <td>green</td>\n",
       "      <td>./protein_pds/HHH_rd4_0200.pkl</td>\n",
       "      <td>HHH_rd4_0200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.959163</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.754055</td>\n",
       "      <td>4.8332</td>\n",
       "      <td>-127.244005</td>\n",
       "      <td>9</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.3956</td>\n",
       "      <td>0.3211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>EHEE</td>\n",
       "      <td>-0.339017</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/EHEE_rd3_0150.pkl</td>\n",
       "      <td>EHEE_rd3_0150</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-2.169682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.743881</td>\n",
       "      <td>8.4475</td>\n",
       "      <td>-86.787270</td>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.9030</td>\n",
       "      <td>0.5982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>EHEE</td>\n",
       "      <td>-0.338700</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/EHEE_rd2_0138.pkl</td>\n",
       "      <td>EHEE_rd2_0138</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-2.093887</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.709368</td>\n",
       "      <td>10.7777</td>\n",
       "      <td>-83.755489</td>\n",
       "      <td>9</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.2715</td>\n",
       "      <td>0.6825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5218</th>\n",
       "      <td>EHEE</td>\n",
       "      <td>-0.338568</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/EHEE_rd1_0979.pkl</td>\n",
       "      <td>EHEE_rd1_0979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.078602</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.690610</td>\n",
       "      <td>11.1954</td>\n",
       "      <td>-83.144086</td>\n",
       "      <td>9</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.8579</td>\n",
       "      <td>0.7602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5219</th>\n",
       "      <td>EHEE</td>\n",
       "      <td>-0.338040</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/EHEE_rd2_0847.pkl</td>\n",
       "      <td>EHEE_rd2_0847</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.209677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684763</td>\n",
       "      <td>10.6580</td>\n",
       "      <td>-88.387069</td>\n",
       "      <td>7</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.5946</td>\n",
       "      <td>0.7080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5220</th>\n",
       "      <td>EHEE</td>\n",
       "      <td>-0.335479</td>\n",
       "      <td>red</td>\n",
       "      <td>./protein_pds/EHEE_rd1_0453.pkl</td>\n",
       "      <td>EHEE_rd1_0453</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-2.094473</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.734145</td>\n",
       "      <td>10.5686</td>\n",
       "      <td>-83.778905</td>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.9704</td>\n",
       "      <td>0.9289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5221 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     topology  stabilityscore_cnn_calibrated  label  \\\n",
       "0         HHH                       2.675084  green   \n",
       "1         HHH                       2.663947  green   \n",
       "2         HHH                       2.614201  green   \n",
       "3         HHH                       2.613560  green   \n",
       "4         HHH                       2.610852  green   \n",
       "...       ...                            ...    ...   \n",
       "5216     EHEE                      -0.339017    red   \n",
       "5217     EHEE                      -0.338700    red   \n",
       "5218     EHEE                      -0.338568    red   \n",
       "5219     EHEE                      -0.338040    red   \n",
       "5220     EHEE                      -0.335479    red   \n",
       "\n",
       "                              pd_path           name  AlaCount  T1_absq  \\\n",
       "0      ./protein_pds/HHH_rd4_0122.pkl   HHH_rd4_0122       1.0      7.0   \n",
       "1      ./protein_pds/HHH_rd4_0395.pkl   HHH_rd4_0395       2.0      6.0   \n",
       "2      ./protein_pds/HHH_rd4_0616.pkl   HHH_rd4_0616       1.0      5.0   \n",
       "3      ./protein_pds/HHH_rd4_0228.pkl   HHH_rd4_0228       2.0      7.0   \n",
       "4      ./protein_pds/HHH_rd4_0200.pkl   HHH_rd4_0200       1.0      8.0   \n",
       "...                               ...            ...       ...      ...   \n",
       "5216  ./protein_pds/EHEE_rd3_0150.pkl  EHEE_rd3_0150       2.0      2.0   \n",
       "5217  ./protein_pds/EHEE_rd2_0138.pkl  EHEE_rd2_0138       2.0      2.0   \n",
       "5218  ./protein_pds/EHEE_rd1_0979.pkl  EHEE_rd1_0979       1.0      2.0   \n",
       "5219  ./protein_pds/EHEE_rd2_0847.pkl  EHEE_rd2_0847       3.0      1.0   \n",
       "5220  ./protein_pds/EHEE_rd1_0453.pkl  EHEE_rd1_0453       2.0      3.0   \n",
       "\n",
       "      T1_netq  Tend_absq  Tend_netq  ...  res_count_core_SCN  score_per_res  \\\n",
       "0        -5.0       12.0       10.0  ...                 8.0      -3.233831   \n",
       "1        -2.0       11.0        7.0  ...                 7.0      -3.185288   \n",
       "2        -5.0       10.0       10.0  ...                 8.0      -3.044371   \n",
       "3        -5.0       11.0        9.0  ...                 7.0      -3.183216   \n",
       "4        -8.0       13.0       13.0  ...                 7.0      -2.959163   \n",
       "...       ...        ...        ...  ...                 ...            ...   \n",
       "5216     -2.0        4.0        4.0  ...                 6.0      -2.169682   \n",
       "5217     -2.0        4.0        2.0  ...                 6.0      -2.093887   \n",
       "5218     -2.0        5.0        3.0  ...                 5.0      -2.078602   \n",
       "5219     -1.0        3.0        3.0  ...                 5.0      -2.209677   \n",
       "5220     -3.0        5.0        3.0  ...                 6.0      -2.094473   \n",
       "\n",
       "      ss_contributes_core     ss_sc  sum_best_frags  total_score  \\\n",
       "0                     1.0  0.778585          5.9791  -139.054743   \n",
       "1                     1.0  0.763438          5.4102  -136.967366   \n",
       "2                     1.0  0.810521          4.7901  -130.907940   \n",
       "3                     1.0  0.800401          5.7477  -136.878274   \n",
       "4                     1.0  0.754055          4.8332  -127.244005   \n",
       "...                   ...       ...             ...          ...   \n",
       "5216                  1.0  0.743881          8.4475   -86.787270   \n",
       "5217                  1.0  0.709368         10.7777   -83.755489   \n",
       "5218                  1.0  0.690610         11.1954   -83.144086   \n",
       "5219                  1.0  0.684763         10.6580   -88.387069   \n",
       "5220                  1.0  0.734145         10.5686   -83.778905   \n",
       "\n",
       "      tryp_cut_sites  two_core_each  worst6frags  worstfrag  \n",
       "0                 12       1.000000       1.6708     0.3360  \n",
       "1                 13       0.666667       1.4301     0.2915  \n",
       "2                 11       1.000000       1.3618     0.2776  \n",
       "3                 11       0.666667       1.7759     0.3556  \n",
       "4                  9       0.666667       1.3956     0.3211  \n",
       "...              ...            ...          ...        ...  \n",
       "5216              10       0.500000       2.9030     0.5982  \n",
       "5217               9       0.500000       3.2715     0.6825  \n",
       "5218               9       0.250000       3.8579     0.7602  \n",
       "5219               7       0.250000       3.5946     0.7080  \n",
       "5220               8       0.500000       3.9704     0.9289  \n",
       "\n",
       "[5221 rows x 114 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stable_proportion = 1  # determines what portion of the stable proteins to use\n",
    "max_num = 10000  # max number of proteins to pick for each topology\n",
    "\n",
    "main_df = pandas.DataFrame(columns = raw_df.columns)\n",
    "for top in topologies_arr:\n",
    "    temp_df = raw_df.groupby('topology').get_group(top)\n",
    "    num_stable = sum(temp_df['stabilityscore_cnn_calibrated'] > stability_threshold)\n",
    "    num_unstable = sum(temp_df['stabilityscore_cnn_calibrated'] <= stability_threshold)\n",
    "    stable_num_to_choose = int(stable_proportion*(num_stable))\n",
    "    if (stable_num_to_choose > max_num):\n",
    "        stable_num_to_choose = max_num  # choose no more than the allowed max\n",
    "    if (num_unstable < stable_num_to_choose):\n",
    "        unstable_num_to_choose = num_unstable  # set max on unstable proteins\n",
    "    else:\n",
    "        unstable_num_to_choose = stable_num_to_choose  \n",
    "    print('Choosing', stable_num_to_choose, 'stable designs out of', num_stable,'for', top)\n",
    "    print('Choosing', unstable_num_to_choose, 'unstable designs out of', num_unstable, 'for', top)\n",
    "    \n",
    "    most_stable = temp_df.nlargest(n=stable_num_to_choose, columns='stabilityscore_cnn_calibrated')\n",
    "    least_stable = temp_df.nsmallest(n=unstable_num_to_choose, columns='stabilityscore_cnn_calibrated')\n",
    "    most_stable = most_stable.reset_index(drop=True)\n",
    "    least_stable = least_stable.reset_index(drop=True)\n",
    "    main_df = pandas.concat([main_df, most_stable, least_stable])\n",
    "    \n",
    "main_df = main_df.reset_index(drop=True)\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split\n",
    "Run multiple iterations for each topology with different train test splits and do grid search over hyperparameters to train a good random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training H_0 CDER model in time 11844.057568073273 seconds\n"
     ]
    }
   ],
   "source": [
    "sme_cder_perf_df = pandas.DataFrame(columns=['topology', 'iteration', 'train_median_num_pairs_retained',\n",
    "                                              'test_median_num_pairs_retained', 'train_accuracy', 'test_accuracy',\n",
    "                                              'roc_auc_test', 'aps', 'confusion_matrix', 'classifier_path', \n",
    "                                              'cder_models_path', 'features_df_path', 'runtime'])\n",
    "filter_type = 'all'  # use all of the persistence pairs from the PD\n",
    "subsample_size = 'all'  # use all of the persistence pairs from the PD\n",
    "num_iterations = 10\n",
    "\n",
    "start_time = time.time()\n",
    "for topology in topologies_arr:\n",
    "    for i in range(num_iterations):\n",
    "        iter_time = time.time()\n",
    "        topology_df = main_df[main_df['topology'] == topology]\n",
    "\n",
    "        # split the data into training and a testing sets\n",
    "        test_size = 0.2\n",
    "        random_state =  i  # change random state for each iteration\n",
    "\n",
    "        train_df = pandas.DataFrame()\n",
    "        test_df = pandas.DataFrame()\n",
    "\n",
    "        for l in ['red', 'green']:\n",
    "            train, test = train_test_split(topology_df[topology_df['label']==l], test_size=test_size, random_state=random_state)\n",
    "            train_df = pandas.concat([train_df, train])\n",
    "            test_df = pandas.concat([test_df, test])\n",
    "\n",
    "        train_df = train_df.reset_index(drop=True)  # drop the index so that the concat won't mix up the labels and the cder output vectors\n",
    "        test_df = test_df.reset_index(drop=True)  # drop the index so that the concat won't mix up the labels and the cder output vectors\n",
    "\n",
    "        train_labels = train_df['label'].to_numpy()\n",
    "        test_labels = test_df['label'].to_numpy()\n",
    "        # load all persistence diagrams by dimension and by train/test split    \n",
    "        train_h_0_all_pds = []\n",
    "        train_h_1_all_pds = []\n",
    "        train_h_2_all_pds = []\n",
    "        test_h_0_all_pds = []\n",
    "        test_h_1_all_pds = []\n",
    "        test_h_2_all_pds = []\n",
    "\n",
    "        # load in raw H_0 diagrams\n",
    "        for path in train_df['pd_path']:\n",
    "            pd_h_0 = np.load(path, allow_pickle=True)[0][:-1,1:]  # remove the infinite persistence pair\n",
    "            train_h_0_all_pds.append(pd_h_0)\n",
    "\n",
    "        for path in test_df['pd_path']:\n",
    "            pd_h_0 = np.load(path, allow_pickle=True)[0][:-1,1:]  # remove the infinite persistence pair\n",
    "            test_h_0_all_pds.append(pd_h_0)\n",
    "\n",
    "        # load in raw H_1 diagrams\n",
    "        for path in train_df['pd_path']:\n",
    "            pd_h_1 = np.load(path, allow_pickle=True)[1]\n",
    "            train_h_1_all_pds.append(pd_h_1)\n",
    "\n",
    "        for path in test_df['pd_path']:\n",
    "            pd_h_1 = np.load(path, allow_pickle=True)[1]\n",
    "            test_h_1_all_pds.append(pd_h_1)\n",
    "\n",
    "        # load in raw H_2 diagrams\n",
    "        for path in train_df['pd_path']:\n",
    "            pd_h_2 = np.load(path, allow_pickle=True)[2]\n",
    "            train_h_2_all_pds.append(pd_h_2)\n",
    "\n",
    "        for path in test_df['pd_path']:\n",
    "            pd_h_2 = np.load(path, allow_pickle=True)[2]\n",
    "            test_h_2_all_pds.append(pd_h_2)\n",
    "\n",
    "        # Get all training and testing PDs together by homology class\n",
    "        train_proc_h_0_all_pds, num_h_0_pairs_retained_train = preprocess_pd_array(train_h_0_all_pds, skew=True, filter=filter_type, t=subsample_size)\n",
    "        train_proc_h_1_all_pds, num_h_1_pairs_retained_train = preprocess_pd_array(train_h_1_all_pds, skew=True, filter=filter_type, t=subsample_size)\n",
    "        train_proc_h_2_all_pds, num_h_2_pairs_retained_train = preprocess_pd_array(train_h_2_all_pds, skew=True, filter=filter_type, t=subsample_size)\n",
    "        test_proc_h_0_all_pds, num_h_0_pairs_retained_test = preprocess_pd_array(test_h_0_all_pds, skew=True, filter=filter_type, t=subsample_size)\n",
    "        test_proc_h_1_all_pds, num_h_1_pairs_retained_test = preprocess_pd_array(test_h_1_all_pds, skew=True, filter=filter_type, t=subsample_size)\n",
    "        test_proc_h_2_all_pds, num_h_2_pairs_retained_test = preprocess_pd_array(test_h_2_all_pds, skew=True, filter=filter_type, t=subsample_size)\n",
    "\n",
    "        # Calculate the median number of persistence pairs retained for train/test data for each shape class\n",
    "        pairs_retained_train_df = pandas.DataFrame(train_df['label'])\n",
    "        pairs_retained_train_df = pairs_retained_train_df.reset_index(drop=True)\n",
    "        pairs_retained_train_df = pandas.concat([pairs_retained_train_df,\n",
    "                                                 pandas.DataFrame(num_h_0_pairs_retained_train, columns=['train_h_0_pairs']),\n",
    "                                                 pandas.DataFrame(num_h_1_pairs_retained_train, columns=['train_h_1_pairs']),\n",
    "                                                 pandas.DataFrame(num_h_2_pairs_retained_train, columns=['train_h_2_pairs'])],\n",
    "                                                axis=1)\n",
    "\n",
    "        pairs_retained_test_df = pandas.DataFrame(test_df['label'])\n",
    "        pairs_retained_test_df = pairs_retained_test_df.reset_index(drop=True)\n",
    "        pairs_retained_test_df = pandas.concat([pairs_retained_test_df,\n",
    "                                                 pandas.DataFrame(num_h_0_pairs_retained_test, columns=['test_h_0_pairs']),\n",
    "                                                 pandas.DataFrame(num_h_1_pairs_retained_test, columns=['test_h_1_pairs']),\n",
    "                                                 pandas.DataFrame(num_h_2_pairs_retained_test, columns=['test_h_2_pairs'])],\n",
    "                                                axis=1)\n",
    "\n",
    "        median_train_pairs_retained_dict = pairs_retained_train_df.groupby('label').median(numeric_only=True).to_dict()\n",
    "        median_test_pairs_retained_dict = pairs_retained_test_df.groupby('label').median(numeric_only=True).to_dict()\n",
    "\n",
    "        # Train CDER models\n",
    "        models_path = 'cder_models/'\n",
    "        h0_file = models_path + f'{topology}_sme_cder_h0_model_iteration_{i}.pickle'\n",
    "        h1_file = models_path + f'{topology}_sme_cder_h1_model_iteration_{i}.pickle'\n",
    "        h2_file = models_path + f'{topology}_sme_cder_h2_model_iteration_{i}.pickle'\n",
    "        tic = time.time()\n",
    "        _, h0_proc_pd_cder_model = gen_cder_model(train_proc_h_0_all_pds, train_labels, \n",
    "                                                  stop_level=None, parsimonious=True,\n",
    "                                                  full_ct=True, out_file=h0_file)\n",
    "        print('Finished training H_0 CDER model in time', time.time()-tic, 'seconds')\n",
    "\n",
    "        tic = time.time()\n",
    "        _, h1_proc_pd_cder_model = gen_cder_model(train_proc_h_1_all_pds, train_labels, \n",
    "                                                  stop_level=None, parsimonious=True,\n",
    "                                                  full_ct=True, out_file=h1_file)\n",
    "        print('Finished training H_1 CDER model in time', time.time()-tic, 'seconds')\n",
    "\n",
    "        tic = time.time()\n",
    "        _, h2_proc_pd_cder_model = gen_cder_model(train_proc_h_2_all_pds, train_labels, \n",
    "                                                  stop_level=None, parsimonious=True,\n",
    "                                                  full_ct=True, out_file=h2_file)\n",
    "        print('Finished training H_2 CDER model in time', time.time()-tic, 'seconds')\n",
    "\n",
    "        # Save CDER models\n",
    "        cder_models = {}\n",
    "        cder_models[0] = h0_proc_pd_cder_model\n",
    "        cder_models[1] = h1_proc_pd_cder_model\n",
    "        cder_models[2] = h2_proc_pd_cder_model\n",
    "\n",
    "        # Create cder models into a dataframe\n",
    "        cder_model_0_df = pandas.DataFrame(cder_models[0].gaussians)\n",
    "        cder_model_1_df = pandas.DataFrame(cder_models[1].gaussians)\n",
    "        cder_model_2_df = pandas.DataFrame(cder_models[2].gaussians)\n",
    "\n",
    "        # Create column to uniquely label each gaussian\n",
    "        cder_model_0_df['name'] = 'H_0_'+cder_model_0_df['label']+cder_model_0_df['mean'].apply(round_array).map(str)\n",
    "        cder_model_1_df['name'] = 'H_1_'+cder_model_1_df['label']+cder_model_1_df['mean'].apply(round_array).map(str)\n",
    "        cder_model_2_df['name'] = 'H_2_'+cder_model_2_df['label']+cder_model_2_df['mean'].apply(round_array).map(str)\n",
    "        \n",
    "        # Create dataframe with each row as the H_0, H_1, and H_2 cder evaluation outputs\n",
    "        train_h_0_raw_evals , _ , _ , = cder_models[0].runit(train_proc_h_0_all_pds)\n",
    "        train_h_1_raw_evals , _ , _ , = cder_models[1].runit(train_proc_h_1_all_pds)\n",
    "        train_h_2_raw_evals , _ , _ , = cder_models[2].runit(train_proc_h_2_all_pds)\n",
    "\n",
    "        test_h_0_raw_evals , _ , _ , = cder_models[0].runit(test_proc_h_0_all_pds)\n",
    "        test_h_1_raw_evals , _ , _ , = cder_models[1].runit(test_proc_h_1_all_pds)\n",
    "        test_h_2_raw_evals , _ , _ , = cder_models[2].runit(test_proc_h_2_all_pds)\n",
    "\n",
    "        temp_df = pandas.DataFrame(columns = ['true_label'])\n",
    "        temp_df['true_label'] = train_df['label']\n",
    "        temp_df = temp_df.reset_index(drop=True)  # drop the index so that the concat in the next lines will not concat on index and mix up the labels and the cder output vectors\n",
    "        train_cder_output_df = pandas.concat([temp_df, \n",
    "                       pandas.concat([pandas.DataFrame(train_h_0_raw_evals, columns=pandas.Index(cder_model_0_df['name'])), \n",
    "                                      pandas.DataFrame(train_h_1_raw_evals, columns=pandas.Index(cder_model_1_df['name'])), \n",
    "                                      pandas.DataFrame(train_h_2_raw_evals, columns=pandas.Index(cder_model_2_df['name']))], axis=1)],\n",
    "                      axis=1)\n",
    "\n",
    "        temp_df = pandas.DataFrame(columns = ['true_label'])\n",
    "        temp_df['true_label'] = test_df['label']\n",
    "        temp_df = temp_df.reset_index(drop=True)  # drop the index so that the concat in the next lines will not concat on index and mix up the labels and the cder output vectors\n",
    "        test_cder_output_df = pandas.concat([temp_df, \n",
    "                       pandas.concat([pandas.DataFrame(test_h_0_raw_evals, columns=pandas.Index(cder_model_0_df['name'])), \n",
    "                                      pandas.DataFrame(test_h_1_raw_evals, columns=pandas.Index(cder_model_1_df['name'])), \n",
    "                                      pandas.DataFrame(test_h_2_raw_evals, columns=pandas.Index(cder_model_2_df['name']))], axis=1)],\n",
    "                      axis=1)\n",
    "\n",
    "        # Rescale and standardize the CDER features\n",
    "        feature_scaler = StandardScaler()\n",
    "        train_cder_standardized_features = pandas.DataFrame(feature_scaler.fit_transform(train_cder_output_df.drop('true_label', axis=1)))\n",
    "        train_cder_standardized_features = pandas.concat((train_cder_output_df['true_label'], train_cder_standardized_features), axis=1)\n",
    "        test_cder_standardized_features = pandas.DataFrame(feature_scaler.fit_transform(test_cder_output_df.drop('true_label', axis=1)))\n",
    "        test_cder_standardized_features = pandas.concat((test_cder_output_df['true_label'], test_cder_standardized_features), axis=1)\n",
    "\n",
    "        # Rename columns to be more descriptive\n",
    "        train_cder_standardized_features.columns = train_cder_output_df.columns\n",
    "        test_cder_standardized_features.columns = test_cder_output_df.columns\n",
    "\n",
    "        # Rescale and standardize the SME features\n",
    "        feature_scaler = StandardScaler()\n",
    "        train_sme_standardized_features = pandas.DataFrame(feature_scaler.fit_transform(train_df.iloc[:, 5:]))\n",
    "        train_sme_standardized_features = pandas.concat((train_df.iloc[:, :5], train_sme_standardized_features), axis=1)\n",
    "        test_sme_standardized_features = pandas.DataFrame(feature_scaler.fit_transform(test_df.iloc[:, 5:]))\n",
    "        test_sme_standardized_features = pandas.concat((test_df.iloc[:, :5], test_sme_standardized_features), axis=1)\n",
    "        \n",
    "        # Rename columns to have original column names\n",
    "        train_sme_standardized_features.columns = list(main_df.columns)\n",
    "        test_sme_standardized_features.columns = list(main_df.columns) \n",
    "        \n",
    "        # Train and test ML model\n",
    "        X_train = train_cder_standardized_features.drop('true_label', axis=1)\n",
    "        y_train = train_cder_standardized_features['true_label']\n",
    "        X_test = test_cder_standardized_features.drop('true_label', axis=1)\n",
    "        y_test = test_cder_standardized_features['true_label']\n",
    "        \n",
    "        # Combine CDER features with SME features\n",
    "        X_train = pandas.concat([X_train, train_sme_standardized_features.iloc[:,5:]], axis=1)\n",
    "        X_test = pandas.concat([X_test, test_sme_standardized_features.iloc[:,5:]], axis=1)\n",
    "        \n",
    "        # Cast column names to strings for classifier\n",
    "        X_train.columns = X_train.columns.astype(str)\n",
    "        X_test.columns = X_test.columns.astype(str)\n",
    "        \n",
    "        # Save the features dataframes\n",
    "        x_train_file = f'features_dataframes/sme_cder_train_X_{topology}_{i}.csv'\n",
    "        y_train_file = f'features_dataframes/sme_cder_train_ylabels_{topology}_{i}.csv'\n",
    "        x_test_file = f'features_dataframes/sme_cder_test_X_{topology}_{i}.csv'\n",
    "        y_test_file = f'features_dataframes/sme_cder_test_ylabels_{topology}_{i}.csv'\n",
    "        X_train.to_csv(x_train_file, index=False)\n",
    "        y_train.to_csv(y_train_file, index=False)\n",
    "        X_test.to_csv(x_test_file, index=False)\n",
    "        y_test.to_csv(y_test_file, index=False)\n",
    "        \n",
    "        # =================== RF =========================\n",
    "        # perform randomized search over rf hyperparameters\n",
    "        \n",
    "        # relabel classes from CDER colors to binary labels\n",
    "        bin_labels_train = np.array([1 if label == 'green' else 0 for label in y_train])\n",
    "        bin_labels_test = np.array([1 if label == 'green' else 0 for label in y_test])\n",
    "\n",
    "        rf_clf = RandomForestClassifier(n_estimators=1000, class_weight='balanced')\n",
    "        max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "        max_depth.append(None)\n",
    "        rf_param_grid = {'max_features': sps.uniform, \n",
    "                         'max_depth': max_depth,\n",
    "                         'min_samples_split': [2, 5, 10, 20, 30],\n",
    "                         'min_samples_leaf': [1, 2, 4, 6, 8, 10]}\n",
    "\n",
    "        rf_clf_gs = model_selection.RandomizedSearchCV(rf_clf, \n",
    "                                                       rf_param_grid,\n",
    "                                                       scoring='average_precision', \n",
    "                                                       n_iter = 100,\n",
    "                                                       cv=10, \n",
    "                                                       n_jobs=-1,\n",
    "                                                       verbose=0)\n",
    "\n",
    "        rf_clf_gs.fit(X_train, bin_labels_train)\n",
    "\n",
    "        print('-- RF best params --')\n",
    "        print(rf_clf_gs.best_params_)\n",
    "\n",
    "        print('-- RF best APS --')\n",
    "        print(rf_clf_gs.best_score_)\n",
    "\n",
    "        classifier_path = f'classifiers/sme_cder_{topology}_rf_clf_gs_{i}.pickle'\n",
    "        with open(classifier_path, 'wb') as f:\n",
    "            pickle.dump(rf_clf_gs, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "        # Get training accuracy\n",
    "        y_train_pred = rf_clf_gs.predict(X_train)\n",
    "\n",
    "        # Make predictions on test data using classifier\n",
    "        y_pred = rf_clf_gs.predict(X_test)\n",
    "\n",
    "        # Prepare performance metrics\n",
    "        train_acc = metrics.accuracy_score(bin_labels_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(bin_labels_test, y_pred)\n",
    "        roc_auc = metrics.roc_auc_score(bin_labels_test, rf_clf_gs.predict_proba(X_test)[:, 1])\n",
    "        aps = metrics.average_precision_score(bin_labels_test, rf_clf_gs.predict_proba(X_test)[:, 1])\n",
    "        cm = confusion_matrix(bin_labels_test, y_pred)\n",
    "        \n",
    "        iter_runtime = time.time() - iter_time\n",
    "\n",
    "        # Create row for sme_cder_perf_df\n",
    "        row = {'topology': topology,\n",
    "               'iteration': i, \n",
    "               'subsample_size': subsample_size, \n",
    "               'train_median_num_pairs_retained': median_train_pairs_retained_dict,\n",
    "               'test_median_num_pairs_retained': median_test_pairs_retained_dict,\n",
    "               'train_accuracy': train_acc, \n",
    "               'test_accuracy': test_acc,\n",
    "               'roc_auc_test': roc_auc,\n",
    "               'aps': aps,\n",
    "               'confusion_matrix': cm,\n",
    "               'classifier_path': classifier_path, \n",
    "               'cder_models_path': [h0_file, h1_file, h2_file],\n",
    "               'features_df_path': [x_train_file, y_train_file, x_test_file, y_test_file],\n",
    "               'runtime': iter_runtime}\n",
    "        sme_cder_perf_df.loc[len(sme_cder_perf_df.index)] = row\n",
    "\n",
    "        sme_cder_perf_df.to_csv(f'./perf_dataframes/sme_cder_perf_df_downsample.csv', index=False)\n",
    "        print(f'Updated and saved sme_cder_perf_df_downsample for this iteration in time {iter_runtime:.2f} seconds')\n",
    "\n",
    "runtime = time.time() - start_time\n",
    "print('Total runtime was', f'{runtime:.2f}', 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train one model without doing a train/test split for assessing correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sme_cder_perf_df = pandas.DataFrame(columns=['topology', 'train_median_num_pairs_retained', 'train_accuracy',\n",
    "                                              'classifier_path', 'cder_models_path', 'features_df_path', 'runtime'])\n",
    "\n",
    "filter_type = 'all'  # use all of the persistence pairs from the PD\n",
    "subsample_size = 'all'  # use all of the persistence pairs from the PD\n",
    "\n",
    "start_time = time.time()\n",
    "for topology in topologies_arr:\n",
    "    iter_time = time.time()\n",
    "    train_df = main_df[main_df['topology'] == topology]\n",
    "    train_df = train_df.reset_index(drop=True)  # drop the index so that the concat won't mix up the labels and the cder output vectors\n",
    "    train_labels = train_df['label'].to_numpy()\n",
    "\n",
    "    # load all persistence diagrams by dimension  \n",
    "    train_h_0_all_pds = []\n",
    "    train_h_1_all_pds = []\n",
    "    train_h_2_all_pds = []\n",
    "\n",
    "    # load in raw H_0 diagrams\n",
    "    for path in train_df['pd_path']:\n",
    "        pd_h_0 = np.load(path, allow_pickle=True)[0][:-1,1:]  # remove the infinite persistence pair\n",
    "        train_h_0_all_pds.append(pd_h_0)\n",
    "\n",
    "    # load in raw H_1 diagrams\n",
    "    for path in train_df['pd_path']:\n",
    "        pd_h_1 = np.load(path, allow_pickle=True)[1]\n",
    "        train_h_1_all_pds.append(pd_h_1)\n",
    "\n",
    "    # load in raw H_2 diagrams\n",
    "    for path in train_df['pd_path']:\n",
    "        pd_h_2 = np.load(path, allow_pickle=True)[2]\n",
    "        train_h_2_all_pds.append(pd_h_2)\n",
    "\n",
    "\n",
    "    # Get all training PDs together by homology class\n",
    "    train_proc_h_0_all_pds, num_h_0_pairs_retained_train = preprocess_pd_array(train_h_0_all_pds, skew=True, filter=filter_type, t=subsample_size)\n",
    "    train_proc_h_1_all_pds, num_h_1_pairs_retained_train = preprocess_pd_array(train_h_1_all_pds, skew=True, filter=filter_type, t=subsample_size)\n",
    "    train_proc_h_2_all_pds, num_h_2_pairs_retained_train = preprocess_pd_array(train_h_2_all_pds, skew=True, filter=filter_type, t=subsample_size)\n",
    "\n",
    "    # Calculate the median number of persistence pairs retained for train data for each shape class\n",
    "    pairs_retained_train_df = pandas.DataFrame(train_df['label'])\n",
    "    pairs_retained_train_df = pairs_retained_train_df.reset_index(drop=True)\n",
    "    pairs_retained_train_df = pandas.concat([pairs_retained_train_df,\n",
    "                                             pandas.DataFrame(num_h_0_pairs_retained_train, columns=['train_h_0_pairs']),\n",
    "                                             pandas.DataFrame(num_h_1_pairs_retained_train, columns=['train_h_1_pairs']),\n",
    "                                             pandas.DataFrame(num_h_2_pairs_retained_train, columns=['train_h_2_pairs'])],\n",
    "                                            axis=1)\n",
    "\n",
    "\n",
    "    median_train_pairs_retained_dict = pairs_retained_train_df.groupby('label').median(numeric_only=True).to_dict()\n",
    "\n",
    "    # Train CDER models\n",
    "    models_path = 'cder_models/'\n",
    "    h0_file = models_path + f'{topology}_sme_cder_h0_model_no_train_test_split.pickle'\n",
    "    h1_file = models_path + f'{topology}_sme_cder_h1_model_no_train_test_split.pickle'\n",
    "    h2_file = models_path + f'{topology}_sme_cder_h2_model_no_train_test_split.pickle'\n",
    "    tic = time.time()\n",
    "    _, h0_proc_pd_cder_model = gen_cder_model(train_proc_h_0_all_pds, train_labels, \n",
    "                                              stop_level=None, parsimonious=True,\n",
    "                                              full_ct=True, out_file=h0_file)\n",
    "    print('Finished training H_0 CDER model in time', time.time()-tic, 'seconds')\n",
    "\n",
    "    tic = time.time()\n",
    "    _, h1_proc_pd_cder_model = gen_cder_model(train_proc_h_1_all_pds, train_labels, \n",
    "                                              stop_level=None, parsimonious=True,\n",
    "                                              full_ct=True, out_file=h1_file)\n",
    "    print('Finished training H_1 CDER model in time', time.time()-tic, 'seconds')\n",
    "\n",
    "    tic = time.time()\n",
    "    _, h2_proc_pd_cder_model = gen_cder_model(train_proc_h_2_all_pds, train_labels, \n",
    "                                              stop_level=None, parsimonious=True,\n",
    "                                              full_ct=True, out_file=h2_file)\n",
    "    print('Finished training H_2 CDER model in time', time.time()-tic, 'seconds')\n",
    "\n",
    "    # Save CDER models\n",
    "    cder_models = {}\n",
    "    cder_models[0] = h0_proc_pd_cder_model\n",
    "    cder_models[1] = h1_proc_pd_cder_model\n",
    "    cder_models[2] = h2_proc_pd_cder_model\n",
    "\n",
    "    # Create cder models into a dataframe\n",
    "    cder_model_0_df = pandas.DataFrame(cder_models[0].gaussians)\n",
    "    cder_model_1_df = pandas.DataFrame(cder_models[1].gaussians)\n",
    "    cder_model_2_df = pandas.DataFrame(cder_models[2].gaussians)\n",
    "\n",
    "    # Create column to uniquely label each gaussian\n",
    "    cder_model_0_df['name'] = 'H_0_'+cder_model_0_df['label']+cder_model_0_df['mean'].apply(round_array).map(str)\n",
    "    cder_model_1_df['name'] = 'H_1_'+cder_model_1_df['label']+cder_model_1_df['mean'].apply(round_array).map(str)\n",
    "    cder_model_2_df['name'] = 'H_2_'+cder_model_2_df['label']+cder_model_2_df['mean'].apply(round_array).map(str)\n",
    "\n",
    "    # Create dataframe with each row as the H_0, H_1, and H_2 cder evaluation outputs\n",
    "    train_h_0_raw_evals , _ , _ , = cder_models[0].runit(train_proc_h_0_all_pds)\n",
    "    train_h_1_raw_evals , _ , _ , = cder_models[1].runit(train_proc_h_1_all_pds)\n",
    "    train_h_2_raw_evals , _ , _ , = cder_models[2].runit(train_proc_h_2_all_pds)\n",
    "\n",
    "    temp_df = pandas.DataFrame(columns = ['true_label'])\n",
    "    temp_df['true_label'] = train_df['label']\n",
    "    temp_df = temp_df.reset_index(drop=True)  # drop the index so that the concat in the next lines will not concat on index and mix up the labels and the cder output vectors\n",
    "    train_cder_output_df = pandas.concat([temp_df, \n",
    "                   pandas.concat([pandas.DataFrame(train_h_0_raw_evals, columns=pandas.Index(cder_model_0_df['name'])), \n",
    "                                  pandas.DataFrame(train_h_1_raw_evals, columns=pandas.Index(cder_model_1_df['name'])), \n",
    "                                  pandas.DataFrame(train_h_2_raw_evals, columns=pandas.Index(cder_model_2_df['name']))], axis=1)],\n",
    "                  axis=1)\n",
    "\n",
    "\n",
    "    # Rescale and standardize the CDER features\n",
    "    feature_scaler = StandardScaler()\n",
    "    train_cder_standardized_features = pandas.DataFrame(feature_scaler.fit_transform(train_cder_output_df.drop('true_label', axis=1)))\n",
    "    train_cder_standardized_features = pandas.concat((train_cder_output_df['true_label'], train_cder_standardized_features), axis=1)\n",
    "\n",
    "    # Rename columns to be more descriptive\n",
    "    train_cder_standardized_features.columns = train_cder_output_df.columns\n",
    "\n",
    "    # Rescale and standardize the SME features\n",
    "    feature_scaler = StandardScaler()\n",
    "    train_sme_standardized_features = pandas.DataFrame(feature_scaler.fit_transform(train_df.iloc[:, 5:]))\n",
    "    train_sme_standardized_features = pandas.concat((train_df.iloc[:, :5], train_sme_standardized_features), axis=1)\n",
    "\n",
    "    # Rename columns to have original column names\n",
    "    train_sme_standardized_features.columns = list(main_df.columns)\n",
    "\n",
    "    # Train ML model\n",
    "    X_train = train_cder_standardized_features.drop('true_label', axis=1)\n",
    "    y_train = train_cder_standardized_features['true_label']\n",
    "\n",
    "    # Combine CDER features with SME features\n",
    "    X_train = pandas.concat([X_train, train_sme_standardized_features.iloc[:,5:]], axis=1)\n",
    "\n",
    "    # Cast column names to strings for classifier\n",
    "    X_train.columns = X_train.columns.astype(str)\n",
    "\n",
    "    # Save the features dataframes\n",
    "    x_train_file = f'features_dataframes/sme_cder_train_X_{topology}_no_train_test_split.csv'\n",
    "    y_train_file = f'features_dataframes/sme_cder_train_ylabels_{topology}_no_train_test_split.csv'\n",
    "    X_train.to_csv(x_train_file, index=False)\n",
    "    y_train.to_csv(y_train_file, index=False)\n",
    "\n",
    "    # =================== RF =========================\n",
    "    # perform randomized search over rf hyperparameters\n",
    "\n",
    "    # relabel classes from CDER colors to binary labels\n",
    "    bin_labels_train = np.array([1 if label == 'green' else 0 for label in y_train])\n",
    "\n",
    "    rf_clf = RandomForestClassifier(n_estimators=1000, class_weight='balanced')\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "    max_depth.append(None)\n",
    "    rf_param_grid = {'max_features': sps.uniform, \n",
    "                     'max_depth': max_depth,\n",
    "                     'min_samples_split': [2, 5, 10, 20, 30],\n",
    "                     'min_samples_leaf': [1, 2, 4, 6, 8, 10]}\n",
    "\n",
    "    rf_clf_gs = model_selection.RandomizedSearchCV(rf_clf, \n",
    "                                                   rf_param_grid,\n",
    "                                                   scoring='average_precision', \n",
    "                                                   n_iter = 100,\n",
    "                                                   cv=10, \n",
    "                                                   n_jobs=-1,\n",
    "                                                   verbose=0)\n",
    "\n",
    "    rf_clf_gs.fit(X_train, bin_labels_train)\n",
    "\n",
    "    print('-- RF best params --')\n",
    "    print(rf_clf_gs.best_params_)\n",
    "\n",
    "    print('-- RF best APS --')\n",
    "    print(rf_clf_gs.best_score_)\n",
    "\n",
    "    classifier_path = f'classifiers/sme_cder_{topology}_rf_clf_gs_no_train_test_split.pickle'\n",
    "    with open(classifier_path, 'wb') as f:\n",
    "        pickle.dump(rf_clf_gs, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Get training accuracy\n",
    "    y_train_pred = rf_clf_gs.predict(X_train)\n",
    "\n",
    "\n",
    "    # Prepare performance metrics\n",
    "    train_acc = metrics.accuracy_score(bin_labels_train, y_train_pred)\n",
    "    iter_runtime = time.time() - iter_time\n",
    "\n",
    "    # Create row for sme_cder_perf_df\n",
    "    row = {'topology': topology,\n",
    "           'subsample_size': subsample_size, \n",
    "           'train_median_num_pairs_retained': median_train_pairs_retained_dict,\n",
    "           'train_accuracy': train_acc, \n",
    "           'classifier_path': classifier_path, \n",
    "           'cder_models_path': [h0_file, h1_file, h2_file],\n",
    "           'features_df_path': [x_train_file, y_train_file],\n",
    "           'runtime': iter_runtime}\n",
    "    sme_cder_perf_df.loc[len(sme_cder_perf_df.index)] = row\n",
    "\n",
    "    sme_cder_perf_df.to_csv(f'./perf_dataframes/sme_cder_perf_df_downsample_no_train_test_split.csv', index=False)\n",
    "    print(f'Updated and saved sme_cder_perf_df_downsample_no_train_test_split for this iteration in time {iter_runtime:.2f} seconds')\n",
    "\n",
    "runtime = time.time() - start_time\n",
    "print('Total runtime was', f'{runtime:.2f}', 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model on just the CDER features for the no train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- RF best params --\n",
      "{'max_depth': 80, 'max_features': 0.38069565908507386, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "-- RF best APS --\n",
      "0.8239716797495215\n",
      "-- RF best params --\n",
      "{'max_depth': 60, 'max_features': 0.2834160176949767, 'min_samples_leaf': 6, 'min_samples_split': 2}\n",
      "-- RF best APS --\n",
      "0.9850593225402454\n",
      "-- RF best params --\n",
      "{'max_depth': None, 'max_features': 0.2800970219766715, 'min_samples_leaf': 1, 'min_samples_split': 30}\n",
      "-- RF best APS --\n",
      "0.8388855068721555\n",
      "-- RF best params --\n",
      "{'max_depth': 30, 'max_features': 0.36263568394334544, 'min_samples_leaf': 10, 'min_samples_split': 30}\n",
      "-- RF best APS --\n",
      "0.9183818141876149\n"
     ]
    }
   ],
   "source": [
    "for topology in topologies_arr:\n",
    "    # Save the features dataframes\n",
    "    X_data_file = pandas.read_csv(f'features_dataframes/sme_cder_train_X_{topology}_no_train_test_split.csv')\n",
    "    y_train = pandas.read_csv(f'features_dataframes/sme_cder_train_ylabels_{topology}_no_train_test_split.csv')\n",
    "    \n",
    "    # Get just the CDER features\n",
    "    X_train = X_data_file.filter(regex=r'^H_')\n",
    "    \n",
    "    # =================== RF =========================\n",
    "    # perform randomized search over rf hyperparameters\n",
    "\n",
    "    # relabel classes from CDER colors to binary labels\n",
    "    bin_labels_train = np.array([1 if label == 'green' else 0 for label in y_train['true_label']])\n",
    "\n",
    "    rf_clf = RandomForestClassifier(n_estimators=1000, class_weight='balanced')\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "    max_depth.append(None)\n",
    "    rf_param_grid = {'max_features': sps.uniform, \n",
    "                     'max_depth': max_depth,\n",
    "                     'min_samples_split': [2, 5, 10, 20, 30],\n",
    "                     'min_samples_leaf': [1, 2, 4, 6, 8, 10]}\n",
    "\n",
    "    rf_clf_gs = model_selection.RandomizedSearchCV(rf_clf, \n",
    "                                                   rf_param_grid,\n",
    "                                                   scoring='average_precision', \n",
    "                                                   n_iter = 100,\n",
    "                                                   cv=10, \n",
    "                                                   n_jobs=-1,\n",
    "                                                   verbose=0)\n",
    "\n",
    "    rf_clf_gs.fit(X_train, bin_labels_train)\n",
    "\n",
    "    print('-- RF best params --')\n",
    "    print(rf_clf_gs.best_params_)\n",
    "\n",
    "    print('-- RF best APS --')\n",
    "    print(rf_clf_gs.best_score_)\n",
    "\n",
    "    classifier_path = f'classifiers/cder_{topology}_rf_clf_gs_no_train_test_split.pickle'\n",
    "    with open(classifier_path, 'wb') as f:\n",
    "        pickle.dump(rf_clf_gs, f, protocol=pickle.HIGHEST_PROTOCOL)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cder2",
   "language": "python",
   "name": "cder2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
